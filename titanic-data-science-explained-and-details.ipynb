{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alirizaercan/titanic-data-science-explained-and-details?scriptVersionId=167048015\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n***We will focus Titanic dataset in this code. It is very important for all user. We can say it is basic dataset for every user. I will try to review this code from a beginner view. If you are ready, let's go!***\n","metadata":{}},{"cell_type":"markdown","source":"**We will have some parts in this code. These parts include every part of data science life cycle. If you want to be a succesfull data scientist/analyst/engineer, you shouldn't pass these life cycle steps!**","metadata":{}},{"cell_type":"markdown","source":"<font color = 'darkblue'>\nContent:\n\n1. [Problem Definition and Project Planning](#1)\n2. [Import Libraries](#2)   \n3. [Load and Check Data](#3)\n4. [Understand Dataset](#4)\n5. [Variable Description](#5)\n6. [Exploratory Data Analysis](#6)   \n    * [Univariate Variable Analysis(EDA)](#7)\n7. [Basic Data Analysis](#8)\n8. [Data Cleaning](#9)\n    * [Outlier Detection](#10)\n    * [Missing Values](#11)\n9. [Feature Engineering](#12)\n10. [Modeling](#13)\n11. [Prediction and Submission](#14)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n# Problem Definition and Project Planning","metadata":{}},{"cell_type":"markdown","source":"***Problem Definition:***\n\nThe problem at hand is to build a predictive model that can answer the question: \"What sorts of people were more likely to survive the sinking of the Titanic?\" The sinking of the Titanic is a historical event where, due to a collision with an iceberg during its maiden voyage on April 15, 1912, a significant number of passengers lost their lives. The challenge is to analyze passenger data, including attributes such as name, age, gender, socio-economic class, etc., and predict which passengers were more likely to survive based on these characteristics.*\n\n**Project Planning:**\n\n* ***Understanding the Objective:***\n\nWe clearly talked about problem in the problem definition. We will focus to find the most optimum **submission.csv** file.\n\n* ***Data Explanation:***\n\n    ***Data Split:***\n\n    ***The dataset is divided into two groups:***\n\n    ***Training set (train.csv)***: Used to build machine learning models, with the ground truth provided for each passenger.\n\n    ***Test set (test.csv)***: Used to evaluate model performance on unseen data, with no ground truth provided.\n\n    ***Target Variable:***\n\n    *Survival:* Binary variable indicating whether a passenger survived (1) or not (0).\n\n    ***Key Features:***\n\n    *Pclass:* Ticket class representing socio-economic status (1st = Upper, 2nd = Middle, 3rd = Lower).\n    *Sex:* Gender of the passenger.\n    *Age:* Age of the passenger in years.\n    *SibSp:* # of siblings/spouses aboard the Titanic.\n    Parch: # of parents/children aboard the Titanic.\n    *Ticket:* Ticket number.\n    *Fare:* Passenger fare.\n    *Cabin:* Cabin number.\n    *Embarked:* Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).\n\n    ***Variable Notes:***\n\n    *Pclass:* A proxy for socio-economic status (SES).\n    *Age:* Fractional if less than 1, or in the form of xx.5 if estimated.\n    *SibSp and Parch:* Define family relations, specifying siblings, spouses, parents, and children.\n    *Some children traveled only with a nanny, resulting in Parch=0 for them.*\n   \n* ***Import Libraries:***\n    *We will import necessary libraries.*\n* ***Load and Check Data:***\n    *We will load and check data. We can say, we will read csv files.*\n* ***Understand  Dataset:***\n    *We will understand the dataset from dataframe.*\n* ***Variable Description:***\n    *We will review variables according to their situation: categorical or numerical.*\n* ***Exploratory Data Analysis(EDA):***\n    *We will understand the dataset deeper with Exploraty Data Analysis. We will focus univariate relationship. We will visualize the data. If you visualize your data you can understand your data easily.*\n* ***Basic Data Analysis:***\n    *We will do basic data analysis so we can understand dataset from deep side.*\n* ***Data Cleaning:***\n    *We will do data cleaning. This is the most important step for data science. This is %80 of the work in the projects. You cannot overlook this step. We will remove duplicates, missing values, reformat data types, check outlier and remove them, validate, if these steps are necessary.* \n* ***Feature Engineering:***\n    *We will focus feature engineering for the best solution in the feature engineering. You can show your creative side in Feature Engineering.*\n* ***Modelling:***\n    *We will choose the best machine learning model for our data. We should choose a good model for our data. If we find the best model for our dataset, we will get a good score.*\n* ***Submission File:***\n    *We will create our submission file.*\n    \n**Now, we can start our code part according to planning steps. You will try to understand whole steps. If you understand whole steps, you can be very good data scientist. Let's start!**","metadata":{}},{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n# Import Libraries\nWe will need some libraries in this project, we need to import necessary libraries. We didn't choose our model so we will talk about model later. We can add our machine learning model libraries later. We can add 'matplotlib', 'seaborn', 'matplotlib.pyplot', 'Counter', 'warning' libraries right now. I can explain their roles in data science like that: \n\n**NumPy:**\nProvides efficient numerical computation capabilities for arrays and matrices.\n\n**Pandas:**\nOffers high-performance, easy-to-use data structures and data analysis tools for labeled data.\n\n**Matplotlib:**\nCreates various static, animated, and interactive visualizations for data exploration and communication.\n\n**Seaborn:**\nBuilds upon Matplotlib to create high-level statistical graphics with a focus on aesthetics and ease of use.\n\n**from collections import Counter:**\nCreates a dictionary-like object (Counter) that counts the occurrences of elements in an iterable (like a list or string).\n\n**warnings:**\nControls how Python handles warning messages.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-14T20:21:43.396527Z","iopub.execute_input":"2024-03-14T20:21:43.397259Z","iopub.status.idle":"2024-03-14T20:21:46.434295Z","shell.execute_reply.started":"2024-03-14T20:21:43.397216Z","shell.execute_reply":"2024-03-14T20:21:46.433119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n# Load and Check Data\nWe will load and check data in this step.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_PassengerId = test_df[\"PassengerId\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.436951Z","iopub.execute_input":"2024-03-14T20:21:46.437909Z","iopub.status.idle":"2024-03-14T20:21:46.475772Z","shell.execute_reply.started":"2024-03-14T20:21:46.437861Z","shell.execute_reply":"2024-03-14T20:21:46.473609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to assign train_df and test_df in here. It will need in the Feature Engineering!","metadata":{}},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.477671Z","iopub.execute_input":"2024-03-14T20:21:46.478237Z","iopub.status.idle":"2024-03-14T20:21:46.516399Z","shell.execute_reply.started":"2024-03-14T20:21:46.47819Z","shell.execute_reply":"2024-03-14T20:21:46.514961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n# Understand Dataset\nWe can understand dataset with some codes and we can check the dataset.","metadata":{}},{"cell_type":"markdown","source":"We can see the columns of train dataset:","metadata":{}},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.520074Z","iopub.execute_input":"2024-03-14T20:21:46.520548Z","iopub.status.idle":"2024-03-14T20:21:46.528376Z","shell.execute_reply.started":"2024-03-14T20:21:46.520503Z","shell.execute_reply":"2024-03-14T20:21:46.526986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will see the first 10 index and last 10 index in the below codes :","metadata":{}},{"cell_type":"code","source":"train_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.529513Z","iopub.execute_input":"2024-03-14T20:21:46.529903Z","iopub.status.idle":"2024-03-14T20:21:46.561593Z","shell.execute_reply.started":"2024-03-14T20:21:46.529873Z","shell.execute_reply":"2024-03-14T20:21:46.560433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.563304Z","iopub.execute_input":"2024-03-14T20:21:46.563707Z","iopub.status.idle":"2024-03-14T20:21:46.588495Z","shell.execute_reply.started":"2024-03-14T20:21:46.563666Z","shell.execute_reply":"2024-03-14T20:21:46.587221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see statistical details about data in the below code: ","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.590331Z","iopub.execute_input":"2024-03-14T20:21:46.590662Z","iopub.status.idle":"2024-03-14T20:21:46.629785Z","shell.execute_reply.started":"2024-03-14T20:21:46.590633Z","shell.execute_reply":"2024-03-14T20:21:46.628162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n# Variable Description\nIn this step we should understand our dataset variables. If we don't understand our variables, we can't do our job with a good result!\n\n*  **PassengerId:** unique id number to each passenger\n*  **Survived:** passenger survive(1) or died(0)\n*  **Pclass:** passenger class\n*  **Name:** name\n*  **Sex:** gender of passenger \n*  **Age:** age of passenger \n*  **SibSp:** number of siblings/spouses\n*  **Parch:** number of parents/children \n*  **Ticket:** ticket number \n*  **Fare:** amount of money spent on ticket\n*  **Cabin:** cabin category\n*  **Embarked:** port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)","metadata":{}},{"cell_type":"markdown","source":"We can see the detailed info about dataset variables. For example we can see data types with .info() method:","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.631892Z","iopub.execute_input":"2024-03-14T20:21:46.632843Z","iopub.status.idle":"2024-03-14T20:21:46.662712Z","shell.execute_reply.started":"2024-03-14T20:21:46.632796Z","shell.execute_reply":"2024-03-14T20:21:46.66133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* int64(5): PassengerId, Survived, Pclass, SibSp, Pclass\n\n* float64(2): Age, Fare\n\n* object(5): Name, Sex, Ticket, Cabin, Embarked\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n# Exploratory Data Analysis (EDA) \nWe can understand data deeper in Exploratory Data Analysis (EDA). In this step, we will do exploratory data analysis. We will focus to univariate variable analysis. We will do some visualization according to our data. We will seperate data to categorical and numerical variables. Firstly we should look our categorical and numerical variables:","metadata":{}},{"cell_type":"markdown","source":"\n**Categorical Variables:** Survived, Pclass, Name, Sex, SibSp, Parch, Ticket, Cabin, Embarked\n\n**Numerical Variables:** PassengerId, Age, Fare","metadata":{}},{"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n## Univariate Variable Analysis\n\nFirstly we should define univariate variable analysis:\nUnivariate analysis is a fundamental statistical technique used to explore and understand the distribution of a single variable within a dataset. It focuses on summarizing the data, identifying patterns, and describing the characteristics of that single variable.\n\nWe separated the variables at the top. We can start with categorical variables in the below code:\n","metadata":{}},{"cell_type":"markdown","source":"### Categorical Variable\n\nWe will use bar_plot graph for analysis, we will visualize categorical variables. In this categorical variables, we will seperate some of them. We need to seperate because some of them nearly unique so we assigned them like categorical_2.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_df[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (12,4))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.664538Z","iopub.execute_input":"2024-03-14T20:21:46.665029Z","iopub.status.idle":"2024-03-14T20:21:46.674329Z","shell.execute_reply.started":"2024-03-14T20:21:46.664985Z","shell.execute_reply":"2024-03-14T20:21:46.673326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_1 = [\"Survived\",\"Pclass\",\"Sex\",\"SibSp\", \"Parch\",\"Embarked\"]\nfor c in categorical_1:\n    bar_plot(c)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:46.679144Z","iopub.execute_input":"2024-03-14T20:21:46.679776Z","iopub.status.idle":"2024-03-14T20:21:48.128013Z","shell.execute_reply.started":"2024-03-14T20:21:46.679715Z","shell.execute_reply":"2024-03-14T20:21:48.126539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_2 = [\"Name\",\"Ticket\",\"Cabin\"]\nfor c in categorical_2:\n    print(\"{} \\n\".format(train_df[c].value_counts()))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:48.13Z","iopub.execute_input":"2024-03-14T20:21:48.130524Z","iopub.status.idle":"2024-03-14T20:21:48.144296Z","shell.execute_reply.started":"2024-03-14T20:21:48.130471Z","shell.execute_reply":"2024-03-14T20:21:48.14291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical Variable\n\nWe will visualize numerical variable in this step. ","metadata":{}},{"cell_type":"code","source":"def plot_hist(var):\n    plt.figure(figsize = (12,4))\n    plt.hist(train_df[var], bins = 50)\n    plt.xlabel(var)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Hist\".format(var))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:48.14714Z","iopub.execute_input":"2024-03-14T20:21:48.147501Z","iopub.status.idle":"2024-03-14T20:21:48.156389Z","shell.execute_reply.started":"2024-03-14T20:21:48.147462Z","shell.execute_reply":"2024-03-14T20:21:48.15546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric = [\"Fare\", \"Age\",\"PassengerId\"]\nfor n in numeric:\n    plot_hist(n)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:48.157558Z","iopub.execute_input":"2024-03-14T20:21:48.158625Z","iopub.status.idle":"2024-03-14T20:21:49.115584Z","shell.execute_reply.started":"2024-03-14T20:21:48.158592Z","shell.execute_reply":"2024-03-14T20:21:49.114649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n# Basic Data Analysis\n\nWe will do basic data analysis. Basic data analysis serves as the foundation for understanding and extracting valuable insights from raw data. \n\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived","metadata":{}},{"cell_type":"markdown","source":"**Pclass - Survived**","metadata":{}},{"cell_type":"code","source":"train_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.11679Z","iopub.execute_input":"2024-03-14T20:21:49.117645Z","iopub.status.idle":"2024-03-14T20:21:49.133381Z","shell.execute_reply.started":"2024-03-14T20:21:49.117615Z","shell.execute_reply":"2024-03-14T20:21:49.132328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sex - Survived**","metadata":{}},{"cell_type":"code","source":"train_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by=\"Survived\", ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.134773Z","iopub.execute_input":"2024-03-14T20:21:49.135342Z","iopub.status.idle":"2024-03-14T20:21:49.161957Z","shell.execute_reply.started":"2024-03-14T20:21:49.135311Z","shell.execute_reply":"2024-03-14T20:21:49.160587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SibSp - Survived**","metadata":{}},{"cell_type":"code","source":"train_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by=\"SibSp\", ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.163377Z","iopub.execute_input":"2024-03-14T20:21:49.16422Z","iopub.status.idle":"2024-03-14T20:21:49.186443Z","shell.execute_reply.started":"2024-03-14T20:21:49.164179Z","shell.execute_reply":"2024-03-14T20:21:49.185328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Parch - Survived**","metadata":{}},{"cell_type":"code","source":"train_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.188364Z","iopub.execute_input":"2024-03-14T20:21:49.188764Z","iopub.status.idle":"2024-03-14T20:21:49.215227Z","shell.execute_reply.started":"2024-03-14T20:21:49.188704Z","shell.execute_reply":"2024-03-14T20:21:49.213798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n# Data Cleaning\n\nIn the data cleaning, we have the most important step for data science lifecycle. In a lot of project, this step is %80 of the work. We will give importance because of that data cleaning step. We will apply that steps: \n\n* **Outlier Detection**\n\n    We will focus IQR test for outlier detection.\n    \n* **Missing Values**\n\n    We will find and fill missing values.\n    \nIn the end of this part we will visualize some values. We will see correlation matrix.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"10\"></a><br>\n## Outlier Detection\n\nWe will do outlier detection. We have some outlier values in the dataset. If you pass this step, you can't have good score in your model. You shouldn't pass! \n\nYou can find a lot of method for outlier detection like IQR, z-score etc. . We will use IQR test in this dataset.","metadata":{}},{"cell_type":"code","source":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.217319Z","iopub.execute_input":"2024-03-14T20:21:49.218039Z","iopub.status.idle":"2024-03-14T20:21:49.228597Z","shell.execute_reply.started":"2024-03-14T20:21:49.217989Z","shell.execute_reply":"2024-03-14T20:21:49.227371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.loc[detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.229978Z","iopub.execute_input":"2024-03-14T20:21:49.230722Z","iopub.status.idle":"2024-03-14T20:21:49.269114Z","shell.execute_reply.started":"2024-03-14T20:21:49.230683Z","shell.execute_reply":"2024-03-14T20:21:49.26752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We found some outlier values. We need to drop from dataset. You can see code in the below: ","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.276607Z","iopub.execute_input":"2024-03-14T20:21:49.277806Z","iopub.status.idle":"2024-03-14T20:21:49.303541Z","shell.execute_reply.started":"2024-03-14T20:21:49.277724Z","shell.execute_reply":"2024-03-14T20:21:49.30197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can pass another step. We detected outlier values and dropped them!","metadata":{}},{"cell_type":"markdown","source":"<a id = \"11\"></a><br>\n## Missing Values\n\nWe need to check dataset that we have missing values or not! We will check. If we have, we will find them. After that we will find missing values. Some machine learning models can't fit your dataset so you need to handle with missing values! Let's find and fill them!","metadata":{}},{"cell_type":"markdown","source":"### Find Missing Values","metadata":{}},{"cell_type":"code","source":"train_df_len = len(train_df)\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.305978Z","iopub.execute_input":"2024-03-14T20:21:49.306984Z","iopub.status.idle":"2024-03-14T20:21:49.336605Z","shell.execute_reply.started":"2024-03-14T20:21:49.306923Z","shell.execute_reply":"2024-03-14T20:21:49.334857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see some missing value columns.","metadata":{}},{"cell_type":"code","source":"train_df.columns[train_df.isnull().any()]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.338964Z","iopub.execute_input":"2024-03-14T20:21:49.339964Z","iopub.status.idle":"2024-03-14T20:21:49.364611Z","shell.execute_reply.started":"2024-03-14T20:21:49.339907Z","shell.execute_reply":"2024-03-14T20:21:49.362855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the sum of missing values by column.","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.367103Z","iopub.execute_input":"2024-03-14T20:21:49.368122Z","iopub.status.idle":"2024-03-14T20:21:49.391154Z","shell.execute_reply.started":"2024-03-14T20:21:49.368059Z","shell.execute_reply":"2024-03-14T20:21:49.390269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill Missing Values\n\nWe found some missing values. We need to fill them. Let's fill!","metadata":{}},{"cell_type":"code","source":"train_df[train_df[\"Embarked\"].isnull()]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.392518Z","iopub.execute_input":"2024-03-14T20:21:49.393093Z","iopub.status.idle":"2024-03-14T20:21:49.415757Z","shell.execute_reply.started":"2024-03-14T20:21:49.393061Z","shell.execute_reply":"2024-03-14T20:21:49.414843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the missing data rows according to Embarked column in the top. Now, I want to show visualization of \"Embarked\" column with boxplot.","metadata":{}},{"cell_type":"code","source":"train_df.boxplot(column=\"Fare\",by = \"Embarked\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.417145Z","iopub.execute_input":"2024-03-14T20:21:49.418059Z","iopub.status.idle":"2024-03-14T20:21:49.66636Z","shell.execute_reply.started":"2024-03-14T20:21:49.418024Z","shell.execute_reply":"2024-03-14T20:21:49.664971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can fill according to boxplot table.","metadata":{}},{"cell_type":"code","source":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.667899Z","iopub.execute_input":"2024-03-14T20:21:49.668256Z","iopub.status.idle":"2024-03-14T20:21:49.683051Z","shell.execute_reply.started":"2024-03-14T20:21:49.668224Z","shell.execute_reply":"2024-03-14T20:21:49.681857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fill \"Age\" column! Let's fill!","metadata":{}},{"cell_type":"code","source":"train_df[train_df[\"Age\"].isnull()]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.684708Z","iopub.execute_input":"2024-03-14T20:21:49.685096Z","iopub.status.idle":"2024-03-14T20:21:49.708961Z","shell.execute_reply.started":"2024-03-14T20:21:49.685065Z","shell.execute_reply":"2024-03-14T20:21:49.708032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to see relationship of \"Age\" feature with other features. We will create correlation matrix in here:","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train_df[[\"Age\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:49.715653Z","iopub.execute_input":"2024-03-14T20:21:49.716276Z","iopub.status.idle":"2024-03-14T20:21:50.013388Z","shell.execute_reply.started":"2024-03-14T20:21:49.716243Z","shell.execute_reply":"2024-03-14T20:21:50.011875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fill according to correlation relationship. Considering the correlation with these features, we can follow a method like the one below to fill in the missing values!","metadata":{}},{"cell_type":"code","source":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"])& (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:50.015266Z","iopub.execute_input":"2024-03-14T20:21:50.015706Z","iopub.status.idle":"2024-03-14T20:21:50.51161Z","shell.execute_reply.started":"2024-03-14T20:21:50.015663Z","shell.execute_reply":"2024-03-14T20:21:50.510649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df[\"Age\"].isnull()]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:50.51299Z","iopub.execute_input":"2024-03-14T20:21:50.514194Z","iopub.status.idle":"2024-03-14T20:21:50.528545Z","shell.execute_reply.started":"2024-03-14T20:21:50.51415Z","shell.execute_reply":"2024-03-14T20:21:50.526419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We filled all missing values for 'Age' column. We can pass Feature Engineering step!","metadata":{}},{"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n# Feature Engineering\n\nWe will focus feature engineering for the best solution in the feature engineering. You can show your creative side in Feature Engineering. Let's show creative side in here!","metadata":{}},{"cell_type":"markdown","source":"## Title & Is Married\n\n**Title** is created by extracting the prefix before **Name** feature. According to graph below, there are many titles that are occuring very few times. Some of those titles doesn't seem correct and they need to be replaced. **Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona** titles are replaced with **Miss/Mrs/Ms** because all of them are female. Values like **Mlle, Mme and Dona** are actually the name of the passengers, but they are classified as titles because **Name** feature is split by comma. **Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev** titles are replaced with **Dr/Military/Noble/Clergy** because those passengers have similar characteristics. **Master** is a *unique* title. It is given to male passengers below age 26. They have the highest survival rate among all males.\n\n*Is_Married* is a binary feature based on the *Mrs* title. *Mrs* title has the highest survival rate among other female titles. This title needs to be a feature because all female titles are grouped with each other.","metadata":{}},{"cell_type":"code","source":"train_df['Title'] = train_df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ntrain_df['Is_Married'] = 0\ntrain_df['Is_Married'].loc[train_df['Title'] == 'Mrs'] = 1","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:50.530807Z","iopub.execute_input":"2024-03-14T20:21:50.531738Z","iopub.status.idle":"2024-03-14T20:21:50.551238Z","shell.execute_reply.started":"2024-03-14T20:21:50.531686Z","shell.execute_reply":"2024-03-14T20:21:50.549833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:50.553255Z","iopub.execute_input":"2024-03-14T20:21:50.553708Z","iopub.status.idle":"2024-03-14T20:21:50.588175Z","shell.execute_reply.started":"2024-03-14T20:21:50.553664Z","shell.execute_reply":"2024-03-14T20:21:50.586909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=train_df['Title'].value_counts().index, y=train_df['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ntrain_df['Title'] = train_df['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\ntrain_df['Title'] = train_df['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n\nsns.barplot(x=train_df['Title'].value_counts().index, y=train_df['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:50.589874Z","iopub.execute_input":"2024-03-14T20:21:50.590263Z","iopub.status.idle":"2024-03-14T20:21:51.340458Z","shell.execute_reply.started":"2024-03-14T20:21:50.590224Z","shell.execute_reply":"2024-03-14T20:21:51.338908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Family Size\n**Family Size** is created by adding **SibSp, Parch and 1**. *SibSp* is the count of siblings and spouse, and *Parch* is the count of parents and children. Those columns are added in order to find the total size of families. Adding *1* at the end, is the current passenger.","metadata":{}},{"cell_type":"code","source":"train_df['Family_Size'] = train_df['SibSp'] + train_df['Parch'] + 1\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ntrain_df['Family_Size_Grouped'] = train_df['Family_Size'].map(family_map)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:51.342051Z","iopub.execute_input":"2024-03-14T20:21:51.343152Z","iopub.status.idle":"2024-03-14T20:21:51.37721Z","shell.execute_reply.started":"2024-03-14T20:21:51.343105Z","shell.execute_reply":"2024-03-14T20:21:51.375993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x=\"Family_Size_Grouped\", y=\"Survived\", data=train_df, kind=\"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:51.379191Z","iopub.execute_input":"2024-03-14T20:21:51.379936Z","iopub.status.idle":"2024-03-14T20:21:51.908189Z","shell.execute_reply.started":"2024-03-14T20:21:51.379891Z","shell.execute_reply":"2024-03-14T20:21:51.906945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = \"Family_Size_Grouped\", data = train_df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:51.910549Z","iopub.execute_input":"2024-03-14T20:21:51.91091Z","iopub.status.idle":"2024-03-14T20:21:52.108298Z","shell.execute_reply.started":"2024-03-14T20:21:51.910881Z","shell.execute_reply":"2024-03-14T20:21:52.10702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x = \"Family_Size\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:52.10971Z","iopub.execute_input":"2024-03-14T20:21:52.110126Z","iopub.status.idle":"2024-03-14T20:21:53.107488Z","shell.execute_reply.started":"2024-03-14T20:21:52.11009Z","shell.execute_reply":"2024-03-14T20:21:53.10594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see in the graph, small families can live with a big possibility.","metadata":{}},{"cell_type":"markdown","source":"## Ticket\nThere are too many unique **Ticket** values to analyze, so grouping them up by their frequencies makes things easier.\n\nHow is this feature different than **Family_Size**? Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren't counted as family, but they used the same ticket.\n\nWhy not grouping tickets by their prefixes? If prefixes in **Ticket** feature has any meaning, then they are already captured in **Pclass** or **Embarked** features because that could be the only logical information which can be derived from the **Ticket** feature.","metadata":{}},{"cell_type":"code","source":"train_df['Ticket_Frequency'] = train_df.groupby('Ticket')['Ticket'].transform('count')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.109271Z","iopub.execute_input":"2024-03-14T20:21:53.109961Z","iopub.status.idle":"2024-03-14T20:21:53.118375Z","shell.execute_reply.started":"2024-03-14T20:21:53.109927Z","shell.execute_reply":"2024-03-14T20:21:53.117112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.120408Z","iopub.execute_input":"2024-03-14T20:21:53.120873Z","iopub.status.idle":"2024-03-14T20:21:53.147607Z","shell.execute_reply.started":"2024-03-14T20:21:53.120824Z","shell.execute_reply":"2024-03-14T20:21:53.146767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation\nWe will do feature transformation for these features. Some models don't fit for categorical values. This is the best thing we can do! Let's do!\n### Label Encoding Non-Numerical Features\n**Embarked, Sex, Title and Family_Size_Grouped** are object type, and **Age** and **Fare** features are category type. They are converted to numerical type with **LabelEncoder**. **LabelEncoder** basically labels the classes from 0 to n. This process is necessary for models to learn from those features.","metadata":{}},{"cell_type":"markdown","source":"Firstly, we need to import some libraries for feature transformation! Let's implement!","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.14886Z","iopub.execute_input":"2024-03-14T20:21:53.149844Z","iopub.status.idle":"2024-03-14T20:21:53.243425Z","shell.execute_reply.started":"2024-03-14T20:21:53.149806Z","shell.execute_reply":"2024-03-14T20:21:53.24211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_numeric_features = ['Embarked', 'Sex', 'Title', 'Family_Size_Grouped','Age', 'Fare']\n\nlabel_encoder = LabelEncoder()\n\nfor column in non_numeric_features:\n    train_df[column] = label_encoder.fit_transform(train_df[column])","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.245625Z","iopub.execute_input":"2024-03-14T20:21:53.246057Z","iopub.status.idle":"2024-03-14T20:21:53.257022Z","shell.execute_reply.started":"2024-03-14T20:21:53.246025Z","shell.execute_reply":"2024-03-14T20:21:53.255801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One-Hot Encoding the Categorical Features\nThe categorical features (**Pclass, Sex, Deck, Embarked, Title**) are converted to one-hot encoded features with **OneHotEncoder**. **Age** and **Fare** features are not converted because they are ordinal unlike the previous ones.","metadata":{}},{"cell_type":"code","source":"cat_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'Family_Size_Grouped']\none_hot_encoder = OneHotEncoder()\nencoded_features = one_hot_encoder.fit_transform(train_df[cat_features]).toarray()\n\ncolumn_names = []\nfor i, column in enumerate(cat_features):\n    unique_labels = train_df[column].unique()\n    \n    names = [f\"{column}_{label}\" for label in unique_labels]\n    column_names.extend(names)\n\none_hot_encoded_df = pd.DataFrame(encoded_features, columns=column_names)\n\ntrain_df = pd.concat([train_df, one_hot_encoded_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.258522Z","iopub.execute_input":"2024-03-14T20:21:53.258891Z","iopub.status.idle":"2024-03-14T20:21:53.282573Z","shell.execute_reply.started":"2024-03-14T20:21:53.258862Z","shell.execute_reply":"2024-03-14T20:21:53.281097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.284581Z","iopub.execute_input":"2024-03-14T20:21:53.28507Z","iopub.status.idle":"2024-03-14T20:21:53.340865Z","shell.execute_reply.started":"2024-03-14T20:21:53.285022Z","shell.execute_reply":"2024-03-14T20:21:53.339794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop Passenger ID and Cabin\nWe need to drop 'PassengerId' and 'Column' columns. We don't need that columns. They are unnecessary for models! ","metadata":{}},{"cell_type":"code","source":"train_df.drop(labels = [\"PassengerId\", \"Cabin\", \"Name\", \"Ticket\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.342848Z","iopub.execute_input":"2024-03-14T20:21:53.344059Z","iopub.status.idle":"2024-03-14T20:21:53.351943Z","shell.execute_reply.started":"2024-03-14T20:21:53.34402Z","shell.execute_reply":"2024-03-14T20:21:53.350576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.353884Z","iopub.execute_input":"2024-03-14T20:21:53.35431Z","iopub.status.idle":"2024-03-14T20:21:53.39Z","shell.execute_reply.started":"2024-03-14T20:21:53.354278Z","shell.execute_reply":"2024-03-14T20:21:53.388823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.39115Z","iopub.execute_input":"2024-03-14T20:21:53.391501Z","iopub.status.idle":"2024-03-14T20:21:53.399053Z","shell.execute_reply.started":"2024-03-14T20:21:53.391472Z","shell.execute_reply":"2024-03-14T20:21:53.397823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to do last thing before model!","metadata":{}},{"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n# Modeling\nWe will put model for our dataset. We completed all processes for our dataset! Firstly we will seperate train-test split.","metadata":{}},{"cell_type":"markdown","source":"We need to import some libraries for machine learning models!","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:53.400569Z","iopub.execute_input":"2024-03-14T20:21:53.400929Z","iopub.status.idle":"2024-03-14T20:21:54.02631Z","shell.execute_reply.started":"2024-03-14T20:21:53.4009Z","shell.execute_reply":"2024-03-14T20:21:54.025067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train - Test Split","metadata":{}},{"cell_type":"code","source":"test = train_df[train_df_len:]\ntest.drop(\"Survived\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:54.027723Z","iopub.execute_input":"2024-03-14T20:21:54.028106Z","iopub.status.idle":"2024-03-14T20:21:54.035505Z","shell.execute_reply.started":"2024-03-14T20:21:54.028077Z","shell.execute_reply":"2024-03-14T20:21:54.034206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:54.037187Z","iopub.execute_input":"2024-03-14T20:21:54.037532Z","iopub.status.idle":"2024-03-14T20:21:54.077139Z","shell.execute_reply.started":"2024-03-14T20:21:54.037504Z","shell.execute_reply":"2024-03-14T20:21:54.075957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_df[:train_df_len]\nX_train = train.drop(\"Survived\", axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(test_df))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:54.078643Z","iopub.execute_input":"2024-03-14T20:21:54.07954Z","iopub.status.idle":"2024-03-14T20:21:54.091094Z","shell.execute_reply.started":"2024-03-14T20:21:54.079505Z","shell.execute_reply":"2024-03-14T20:21:54.089732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    'Logistic Regression': LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'SVM': SVC(),\n    'KNN': KNeighborsClassifier(),\n    'XGBoost': xgb.XGBClassifier()\n}\n\nfor model_name, model in models.items():\n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = model.predict(X_test)\n    \n    # Evaluate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f'{model_name}: Accuracy = {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:54.092782Z","iopub.execute_input":"2024-03-14T20:21:54.09322Z","iopub.status.idle":"2024-03-14T20:21:54.815411Z","shell.execute_reply.started":"2024-03-14T20:21:54.093173Z","shell.execute_reply":"2024-03-14T20:21:54.814387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can choose Logistic Regression. Logistic Regression gives the best accuracy_score in the titanic dataset!","metadata":{}},{"cell_type":"markdown","source":"We trained Logistic Regression for titanic dataset in the below code!","metadata":{}},{"cell_type":"code","source":"logistic_regression = LogisticRegression()\nlogistic_regression = logistic_regression.fit(X_train, y_train)\nprint(accuracy_score(logistic_regression.predict(X_test),y_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:54.819659Z","iopub.execute_input":"2024-03-14T20:21:54.822621Z","iopub.status.idle":"2024-03-14T20:21:54.890374Z","shell.execute_reply.started":"2024-03-14T20:21:54.822575Z","shell.execute_reply":"2024-03-14T20:21:54.888836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"14\"></a><br>\n# Prediction and Submit\nWe will predict and submit our .csv file. We can finish this notebook in here!","metadata":{}},{"cell_type":"code","source":"test_survived = pd.Series(logistic_regression.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"submission.csv\",header=True, index = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T20:21:54.897544Z","iopub.execute_input":"2024-03-14T20:21:54.9027Z","iopub.status.idle":"2024-03-14T20:21:54.927875Z","shell.execute_reply.started":"2024-03-14T20:21:54.90262Z","shell.execute_reply":"2024-03-14T20:21:54.926333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope this notebook can be helpful for your data scientist journey!\n\n> Respects,\n> Ali Riza Ercan\n\n                                                                   ","metadata":{}}]}