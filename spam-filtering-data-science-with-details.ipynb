{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":260807,"sourceType":"datasetVersion","datasetId":109196}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alirizaercan/spam-filtering-data-science-with-details?scriptVersionId=183089736\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\nThe primary goal of this notebook is to develop a predictive model that accurately classifies incoming Email messages as either spam or not. We will use the spam-mails-dataset.","metadata":{}},{"cell_type":"markdown","source":"**In this code, there will be some parts. These parts include every part of data science life cycle. If you want to be a succesfull data scientist/analyst/engineer, you shouldn't pass these life cycle steps!**","metadata":{}},{"cell_type":"markdown","source":"<font color='black'>\n\nContent:\n\n1. [Problem Definition and Project Planning](#1)\n2. [Import Libraries](#2)   \n3. [Load and Check Data](#3)\n4. [Understand Dataset](#4)\n5. [Column Description](#5)\n6. [Exploratory Data Analysis](#6)   \n    * [Univariate Variable Analysis(EDA)](#7)\n7. [Basic Data Analysis](#8)\n8. [Data Cleaning](#9)\n    * [Outlier Detection](#10)\n    * [Missing Values](#11)\n9. [Feature Engineering](#12)\n10. [Modeling](#13)","metadata":{}},{"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n# Problem Definition and Project Planning","metadata":{}},{"cell_type":"markdown","source":"## Problem Definition\n\nThis project aims to develop a spam mail detection with classification machine learning methods.\n\n### Project Planning\n\n#### Understanding the Objective\n\nOur main goal is to detect mails spam or not.\n\n#### Data Explanation\n\nThe spam-mails-dataset includes **spam_ham_dataset.csv** file.\n\n**Import Libraries**\n\nWe will import the necessary libraries to perform data analysis and build machine learning models.\n\n**Load and Check Data**\n\nWe will read and check the spam-mails-dataset in CSV format.\n\n**Understand Dataset**\n\nWe will explore the dataset to gain a comprehensive understanding of its structure and contents.\n\n**Column Description**\n\nWe will review columns.\n\n**Exploratory Data Analysis (EDA)**\n\nWe will conduct Exploratory Data Analysis to deepen our understanding of the dataset. This includes visualizing data to uncover patterns and relationships.\n\n**Data Cleaning**\n\nWe will clean the data by addressing issues such as duplicates, missing values, reformatting data types, checking and handling outliers, and validating.\n\n**Feature Engineering**\n\nWe will focus on feature engineering to detect spam mails easily.\n\n**Modelling**\n\nWe will choose the best machine learning model for predicting customer transactions based on the dataset.\n\n**Submission File**\n\nWe will create the submission file, adhering to the required format.\n\nNow, let's proceed with the code implementation according to these planned steps. Understanding each step is crucial for a comprehensive data science approach. Happy coding!","metadata":{}},{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n# Import Libraries\nWe will need some libraries in this project, we need to import necessary libraries. We didn't choose our model so we will talk about model later. We can add our machine learning model libraries later. We can add 'matplotlib', 'seaborn', 'matplotlib.pyplot', 'warning' libraries right now. I can explain their roles in data science like that: \n\n**NumPy:**\nProvides efficient numerical computation capabilities for arrays and matrices.\n\n**Pandas:**\nOffers high-performance, easy-to-use data structures and data analysis tools for labeled data.\n\n**Matplotlib:**\nCreates various static, animated, and interactive visualizations for data exploration and communication.\n\n**Seaborn:**\nBuilds upon Matplotlib to create high-level statistical graphics with a focus on aesthetics and ease of use.\n\n**warnings:**\nControls how Python handles warning messages.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T06:33:13.500516Z","iopub.execute_input":"2024-04-22T06:33:13.50095Z","iopub.status.idle":"2024-04-22T06:33:16.408745Z","shell.execute_reply.started":"2024-04-22T06:33:13.500918Z","shell.execute_reply":"2024-04-22T06:33:16.406999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n# Load and Check Data\nWe will load and check data in this step.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv')\ntest_id = df[\"Unnamed: 0\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.411479Z","iopub.execute_input":"2024-04-22T06:33:16.412278Z","iopub.status.idle":"2024-04-22T06:33:16.608925Z","shell.execute_reply.started":"2024-04-22T06:33:16.412236Z","shell.execute_reply":"2024-04-22T06:33:16.607441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will see dataframe in the below!","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.610939Z","iopub.execute_input":"2024-04-22T06:33:16.611671Z","iopub.status.idle":"2024-04-22T06:33:16.637971Z","shell.execute_reply.started":"2024-04-22T06:33:16.611628Z","shell.execute_reply":"2024-04-22T06:33:16.636716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n# Understand Dataset\nWe can understand dataset with some codes and we can check the dataset.","metadata":{}},{"cell_type":"markdown","source":"We can see the columns of train dataset:","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.641078Z","iopub.execute_input":"2024-04-22T06:33:16.641822Z","iopub.status.idle":"2024-04-22T06:33:16.650218Z","shell.execute_reply.started":"2024-04-22T06:33:16.641781Z","shell.execute_reply":"2024-04-22T06:33:16.648728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will see the first 10 index and last 10 index in the below codes :","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.65202Z","iopub.execute_input":"2024-04-22T06:33:16.652737Z","iopub.status.idle":"2024-04-22T06:33:16.672684Z","shell.execute_reply.started":"2024-04-22T06:33:16.652698Z","shell.execute_reply":"2024-04-22T06:33:16.671531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.67434Z","iopub.execute_input":"2024-04-22T06:33:16.675033Z","iopub.status.idle":"2024-04-22T06:33:16.689629Z","shell.execute_reply.started":"2024-04-22T06:33:16.674995Z","shell.execute_reply":"2024-04-22T06:33:16.688405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see statistical details about data in the below code: ","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.691413Z","iopub.execute_input":"2024-04-22T06:33:16.692189Z","iopub.status.idle":"2024-04-22T06:33:16.725832Z","shell.execute_reply.started":"2024-04-22T06:33:16.692148Z","shell.execute_reply":"2024-04-22T06:33:16.724673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Column Description\nIn this step we should understand our dataset columns. If we don't understand our columns, we can't do our job with a good result!\n\n* **Unnamed: 0 :** This column typically represents the rows that make up a dataframe. In a dataset obtained from Kaggle or another source, this column is usually used as the index column of the dataset. However, in dataset, it seems that this column is not an index column but rather an additional column created during indexing.\n\n* **label:** This column contains the label of each instance. For example, it may contain categorical labels such as \"spam\" or \"ham\". \"Spam\" typically denotes unwanted emails, while \"ham\" denotes normal emails.\n\n* **text:** This column contains the text content of each instance. It's usually the main column for processing text data. It's important for tasks like text classification or natural language processing.\n\n* **label_num:** This column contains the numerical equivalents of the categorical labels. It's often used to encode categorical labels numerically for easier processing by machine learning algorithms. For example, 1 for the \"spam\" label and 0 for the \"ham\" label.","metadata":{}},{"cell_type":"markdown","source":"We can see the detailed info about dataset column. For example we can see data types with .info() method:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.727607Z","iopub.execute_input":"2024-04-22T06:33:16.728302Z","iopub.status.idle":"2024-04-22T06:33:16.754253Z","shell.execute_reply.started":"2024-04-22T06:33:16.728263Z","shell.execute_reply":"2024-04-22T06:33:16.752708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can say these things for data types according to the .info method:\n\n* **Object Variables:**\n  - label: object\n  - text: object\n  \n\n* **Integer Variables:**\n  - Unnamed: 0: int64\n  - label_num: int64","metadata":{}},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n# Exploratory Data Analysis (EDA) \nWe can understand data deeper in Exploratory Data Analysis (EDA). In this step, we will do exploratory data analysis.","metadata":{}},{"cell_type":"markdown","source":"### Numerical Variable\n\nWe will visualize numerical variable in this step. ","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nnumerical_columns = df.select_dtypes(include=['int64']).columns.tolist()\ncategorical_columns = df.select_dtypes(include=['object']).columns.tolist()\nnum_cols = len(numerical_columns)\nnum_rows = -(-num_cols // 4) \n\nplt.figure(figsize=(20, 6 * num_rows))\nfor i, col in enumerate(numerical_columns, 1):\n    plt.subplot(num_rows, 4, i)\n    sns.histplot(df[col], kde=True)\n    plt.title(f'{col} Distribution')\n    plt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:16.756926Z","iopub.execute_input":"2024-04-22T06:33:16.757855Z","iopub.status.idle":"2024-04-22T06:33:18.082672Z","shell.execute_reply.started":"2024-04-22T06:33:16.757812Z","shell.execute_reply":"2024-04-22T06:33:18.081634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Variable\n\nWe will visualize categorical variables! We will visualize only 'label' column because the other column is not distributed normal. It will not be understandable.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nsns.countplot(x='label', data=df)\nplt.title('Label Distribution')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:18.086687Z","iopub.execute_input":"2024-04-22T06:33:18.087399Z","iopub.status.idle":"2024-04-22T06:33:18.370227Z","shell.execute_reply.started":"2024-04-22T06:33:18.08736Z","shell.execute_reply":"2024-04-22T06:33:18.36851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n# Basic Data Analysis\n\nWe will do basic data analysis. Basic data analysis serves as the foundation for understanding and extracting valuable insights from raw data. \n","metadata":{}},{"cell_type":"code","source":"df[[\"label\",\"label_num\"]].groupby([\"label\"], as_index = False).mean().sort_values(by=\"label_num\",ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:18.37192Z","iopub.execute_input":"2024-04-22T06:33:18.372399Z","iopub.status.idle":"2024-04-22T06:33:18.396786Z","shell.execute_reply.started":"2024-04-22T06:33:18.372359Z","shell.execute_reply":"2024-04-22T06:33:18.395262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"text\",\"label_num\"]].groupby([\"text\"], as_index = False).mean().sort_values(by=\"label_num\",ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:18.398131Z","iopub.execute_input":"2024-04-22T06:33:18.398474Z","iopub.status.idle":"2024-04-22T06:33:18.440204Z","shell.execute_reply.started":"2024-04-22T06:33:18.398433Z","shell.execute_reply":"2024-04-22T06:33:18.438774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n# Data Cleaning\n\nIn the data cleaning, we have the most important step for data science lifecycle. In a lot of project, this step is %80 of the work. We will give importance because of that data cleaning step.\n\n","metadata":{}},{"cell_type":"code","source":"df.duplicated().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:18.441717Z","iopub.execute_input":"2024-04-22T06:33:18.442086Z","iopub.status.idle":"2024-04-22T06:33:18.478Z","shell.execute_reply.started":"2024-04-22T06:33:18.442056Z","shell.execute_reply":"2024-04-22T06:33:18.475311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:18.480121Z","iopub.execute_input":"2024-04-22T06:33:18.480608Z","iopub.status.idle":"2024-04-22T06:33:18.49916Z","shell.execute_reply.started":"2024-04-22T06:33:18.480565Z","shell.execute_reply":"2024-04-22T06:33:18.497554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n# Feature Engineering\n\nWe will focus feature engineering for the best solution in the feature engineering. You can show your creative side in Feature Engineering. Let's show creative side in here!\n\nWe will focus NLP processes in here!","metadata":{}},{"cell_type":"markdown","source":"## Bag of Words (BOW) Algorithm\n\n### Introduction\n**The Bag of Words (BOW) model** is a simple yet powerful natural language processing tool used to extract features from text data. This approach converts text into fixed-length vectors by counting how many times each word appears. BOW models are widely used in various applications, including document classification, sentiment analysis, and topic modeling.\n\n### Tokenization and Cleaning\n**Step 1:** Tokenization\n\n**What is it?** Tokenization is the process of splitting text into individual elements, typically words or phrases.\nHow it's done: This involves breaking down a block of text into smaller pieces, such as words or sentences. For example, the sentence \"Hello, how are you?\" can be tokenized into [\"Hello\", \"how\", \"are\", \"you\"].\n**Step 2:** Cleaning\n\nWhat is it? Cleaning involves removing unnecessary characters, such as punctuation, special characters, and numbers. It also includes converting all text to lowercase to ensure uniformity.\n**Techniques used:**\nRemoving stop words (commonly used words such as \"and\", \"the\", etc. that do not contribute much meaning to the sentences).\nLemmatization or stemming, which simplifies words to their base or root form.\n\n\n### Vectorization\nOnce the text has been tokenized and cleaned, the next step is to convert it into a numerical format that a machine learning algorithm can process. This is achieved through vectorization.\n\n**Count Vectors:** Each document is represented by a vector where each dimension corresponds to a word from the entire corpus (collection of all documents), and the value in each dimension represents the frequency of the word in the document.\n**TF-IDF Vectors (Term Frequency-Inverse Document Frequency):** This is a common variation that not only takes into account the frequency of a word in a single document (TF) but also how common the word is across all documents (IDF). This helps to adjust for the fact that some words appear more frequently in general.","metadata":{}},{"cell_type":"code","source":"import nltk\nimport string\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:18.501144Z","iopub.execute_input":"2024-04-22T06:33:18.503161Z","iopub.status.idle":"2024-04-22T06:33:19.615402Z","shell.execute_reply.started":"2024-04-22T06:33:18.503095Z","shell.execute_reply":"2024-04-22T06:33:19.614125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = PorterStemmer()\ncorpus = []\n\nall_stop_words = set(stopwords.words('english'))\nall_stop_words.remove('not')\n\nfor i in range (len(df)):\n    text = df['text'][i].lower().translate(str.maketrans('','', string.punctuation)).split()\n    text = [ps.stem(word) for word in text if word not in all_stop_words]\n    text = ' '.join(text)\n    corpus.append(text)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:19.617229Z","iopub.execute_input":"2024-04-22T06:33:19.618074Z","iopub.status.idle":"2024-04-22T06:33:39.854358Z","shell.execute_reply.started":"2024-04-22T06:33:19.618038Z","shell.execute_reply":"2024-04-22T06:33:39.853004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n# Modeling\nWe will put model for our dataset. We completed all processes for our dataset! Firstly we will seperate train-test split.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier as RFR\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:39.8563Z","iopub.execute_input":"2024-04-22T06:33:39.856858Z","iopub.status.idle":"2024-04-22T06:33:40.05941Z","shell.execute_reply.started":"2024-04-22T06:33:39.856811Z","shell.execute_reply":"2024-04-22T06:33:40.057582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = CountVectorizer(max_features= 42500)\nX = cv.fit_transform(corpus).toarray()\ny = df['label_num']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:40.061323Z","iopub.execute_input":"2024-04-22T06:33:40.062105Z","iopub.status.idle":"2024-04-22T06:33:41.528509Z","shell.execute_reply.started":"2024-04-22T06:33:40.062058Z","shell.execute_reply":"2024-04-22T06:33:41.527442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:41.529941Z","iopub.execute_input":"2024-04-22T06:33:41.530311Z","iopub.status.idle":"2024-04-22T06:33:41.538386Z","shell.execute_reply.started":"2024-04-22T06:33:41.530282Z","shell.execute_reply":"2024-04-22T06:33:41.537113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train - Test Split\nIn the data, we should split data into train and test!","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:41.540028Z","iopub.execute_input":"2024-04-22T06:33:41.540399Z","iopub.status.idle":"2024-04-22T06:33:42.772957Z","shell.execute_reply.started":"2024-04-22T06:33:41.540369Z","shell.execute_reply":"2024-04-22T06:33:42.772061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model\nIn this step. the data will be training the model!","metadata":{}},{"cell_type":"code","source":"def model_score(y_true,y_pred):\n    acc_scor = accuracy_score(y_true, y_pred)\n    prec_scor = precision_score(y_true, y_pred)\n    recall_scor = recall_score(y_true, y_pred)\n    f1_scor = f1_score(y_true, y_pred)\n    overall_avg_score = (acc_scor + prec_scor + recall_scor + f1_scor) / 4\n\n    print(f'Model accuracy score: {acc_scor}')\n    print(f'Model precision score: {prec_scor}')\n    print(f'Model recall score: {recall_scor}')\n    print(f'Model f1 score: {f1_scor}')\n    print(f'Average overall score performance: {overall_avg_score}')\n\n    print(confusion_matrix(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:42.774295Z","iopub.execute_input":"2024-04-22T06:33:42.775299Z","iopub.status.idle":"2024-04-22T06:33:42.782836Z","shell.execute_reply.started":"2024-04-22T06:33:42.775266Z","shell.execute_reply":"2024-04-22T06:33:42.781721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"cl_rf = RFR(n_estimators=100, random_state=42)\ncl_rf.fit(X_train, y_train)\ny_pred = cl_rf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:33:42.783974Z","iopub.execute_input":"2024-04-22T06:33:42.784371Z","iopub.status.idle":"2024-04-22T06:34:09.317811Z","shell.execute_reply.started":"2024-04-22T06:33:42.784325Z","shell.execute_reply":"2024-04-22T06:34:09.31638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.column_stack((y_test[:15], y_pred[:15]))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:34:09.319673Z","iopub.execute_input":"2024-04-22T06:34:09.320035Z","iopub.status.idle":"2024-04-22T06:34:09.330378Z","shell.execute_reply.started":"2024-04-22T06:34:09.320008Z","shell.execute_reply":"2024-04-22T06:34:09.328784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Score","metadata":{}},{"cell_type":"code","source":"model_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:34:09.332945Z","iopub.execute_input":"2024-04-22T06:34:09.333323Z","iopub.status.idle":"2024-04-22T06:34:09.355602Z","shell.execute_reply.started":"2024-04-22T06:34:09.333295Z","shell.execute_reply":"2024-04-22T06:34:09.354334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_obeyes = pd.Series(cl_rf.predict(X_test), name = \"label_num\")\nresults = pd.concat([test_id, test_obeyes],axis = 1)\nresults.to_csv(\"submission.csv\",header=True, index = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:35:08.130052Z","iopub.execute_input":"2024-04-22T06:35:08.130483Z","iopub.status.idle":"2024-04-22T06:35:08.33725Z","shell.execute_reply.started":"2024-04-22T06:35:08.130438Z","shell.execute_reply":"2024-04-22T06:35:08.335783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = pd.read_csv('submission.csv')\nresult_df","metadata":{"execution":{"iopub.status.busy":"2024-04-22T06:35:17.976072Z","iopub.execute_input":"2024-04-22T06:35:17.9766Z","iopub.status.idle":"2024-04-22T06:35:17.99972Z","shell.execute_reply.started":"2024-04-22T06:35:17.976556Z","shell.execute_reply":"2024-04-22T06:35:17.997332Z"},"trusted":true},"execution_count":null,"outputs":[]}]}